# -*- coding: utf-8 -*-
"""Hate Speech Detection using deep learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XG7SY5B34fyg-oSYg3LVP-HGH_EonW-3

# **1. Dataset**

### 1.1 Load the dataset
"""

# load the data
import pandas as pd
dataset = pd.read_csv("/content/drive/MyDrive/L645/data/labeled_data.csv")

"""### 1.2 Data analysis"""

# Number of tweets that are annotated as hate-speech
print(sum(dataset['class'] == 0))

# Number of tweets that are annotated as offensive language
print(sum(dataset['class'] == 1))

# Number of tweets that are annotated as neither hate-speech nor offensive
print(sum(dataset['class'] == 2))

# # Check if there are any null values in the dataset
dataset.isna().sum()

"""### 1.3 Data visualization"""

# 0 for hate speech
# 1 for offensive language
# 2 for neither hate-speech nor offensive

import seaborn as sns
sns.countplot(x = 'class', data = dataset)

"""# **2. Data preprocessing**

### 2.1 Data cleaning
"""

import nltk, re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
nltk.download('stopwords')
nltk.download('punkt')

stop_words = stopwords.words('english')
stop_words_dict = Counter(stop_words)
contract_patterns = [(r'won\'t', 'will not'), (r'can\'t', 'cannot'), (r'i\'m', 'i am'), (r'ain\'t', 'be not'), (r'(\w+)\'ll', '\g<1> will'), (r'(\w+)n\'t', '\g<1> not'),
                        (r'(\w+)\'ve', '\g<1> have'), (r'(\w+)\'s', '\g<1> is'), (r'(\w+)\'re', '\g<1> are'), (r'(\w+)\'d', '\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it')]

# Replace contractions from tweets
def replace_contraction(text):
    patterns = [(re.compile(regex), rpl) for (regex, rpl) in contract_patterns]
    for (pattern, rpl) in patterns:
        (text, count) = re.subn(pattern, rpl, text)
    return text

# Clean the tweets by removing and replacing the unrequired characters
def clean_tweets(tweet):
    tweet = re.sub(r' RT ', ' ', tweet) # Remove RT
    tweet = re.sub(r'http\S+', 'url', tweet) # Replaces url address with "url"
    tweet = re.sub(r'@[^\s]+','atusername',tweet) # Replaces "@user" with "atusername"
    tweet = re.sub(r'#[^\s]+', ' ', tweet) # Removes hastag in front of a word 
    tweet = re.sub(r'[^\w\s]', ' ', tweet) # Remove everything except characters and whitespace
    tweet = re.sub(r'[^a-zA-Z] ', ' ', tweet) # Remove numbers
    tweet = re.sub(r'\s\s+', ' ', tweet) # Remove multiple spaces
    tweet = tweet.lower() # Convert each character to lower case and remove spaces around them
    return tweet

# Removes stop words
def remove_stop_words(tweet):
    tokens = word_tokenize(tweet)
    tokens = [word for word in tokens if word not in stop_words_dict]
    return ' '.join(tokens)

# Replace contractions in the tweets
dataset["tweet"] = dataset.tweet.apply(replace_contraction)

# Clean the tweets
dataset["tweet"] = dataset.tweet.apply(clean_tweets)

# Remove stop words in the tweets
dataset['tweet'] = dataset.tweet.apply(remove_stop_words)

print(dataset['tweet'])

"""### 2.2 Lemmatization or Stemming"""

from nltk.stem import WordNetLemmatizer, PorterStemmer
nltk.download('wordnet')
nltk.download('omw-1.4')

# Lemmatization
def lemmatize_tweets(tweet):
    tokens = word_tokenize(tweet)
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

# Stemming
def stemming_tweets(tweet):
    tokens = word_tokenize(tweet)
    ps = PorterStemmer()
    tokens = [ps.stem(word) for word in tokens]
    return ' '.join(tokens)

# Create word representation
def word_representation(dataset, type = 'lemmatization'):
    if type == 'lemmatization':
        dataset['tweet'] = list(map(lemmatize_tweets, dataset['tweet']))
    else:
        dataset['tweet'] = list(map(stemming_tweets, dataset['tweet']))
    return dataset

dataset = word_representation(dataset)
dataset["tweet"]

"""# **3. Sequence Data Transformation**

### 3.1 Spliting the dataset into train and test set
"""

# Split the train and test dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(dataset['tweet'], dataset['class'], random_state = 42, test_size=0.1, shuffle = True)

"""### 3.2 Data preparation using tokenization, data sequence preparation, padding for equal length sequence."""

# Define Hyperparameters
VOCABULARY_SIZE = 2000
MAX_LENGTH = 60

# Tokenization
from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=VOCABULARY_SIZE, oov_token = '<OOV>')
tokenizer.fit_on_texts(list(X_train) + list(X_test))

# Save the toeknizer
import pickle
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# Sequences
X_train_sequence = tokenizer.texts_to_sequences(X_train)
X_test_sequence = tokenizer.texts_to_sequences(X_test)

# Padding
from keras_preprocessing.sequence import pad_sequences
X_train_padding = pad_sequences(X_train_sequence, maxlen = MAX_LENGTH, padding = 'post', truncating = 'post')
X_test_padding = pad_sequences(X_test_sequence, maxlen = MAX_LENGTH, padding = 'post', truncating = 'post')

word_index = tokenizer.word_index

"""### 3.3 Sequence representation using word embeddings (GloVe, Word2Vec, and Paragram)

### 3.3.1 Golve
"""

# from tqdm import tqdm
# import numpy as np
# glove_embeddings_index = {}
# f = open('/content/drive/MyDrive/L645/data/glove.6B.100d.txt', encoding='utf8') #100d

# for line in tqdm(f):
#     values = line.split()
#     word = values[0]
#     coefs = np.asarray(values[1:],dtype='float32')
#     glove_embeddings_index[word] = coefs
# f.close()

# print('Found %s word vectors.' % len(glove_embeddings_index))

# embedding_matrix = np.zeros((len(word_index)+ 1, 100))
# for word, i in tqdm(word_index.items()):
#     embedding_vector = glove_embeddings_index.get(word)
#     if embedding_vector is not None:
#         embedding_matrix[i] = embedding_vector

"""### 3.3.2 Word2Vec"""

# from tqdm import tqdm
# import numpy as np
# W2V_embeddings_index = {}
# f = open('/content/drive/MyDrive/L645/data/word2vec.txt', encoding='utf8')  #100d

# for line in tqdm(f):
#     values = line.split()
#     word = values[0]
#     coefs = np.asarray(values[1:],dtype='float32')
#     W2V_embeddings_index[word] = coefs
# f.close()

# print('Found %s word vectors.' % len(W2V_embeddings_index))

# embedding_matrix = np.zeros((len(word_index)+ 1, 100))
# for word, i in tqdm(word_index.items()):
#     embedding_vector = W2V_embeddings_index.get(word)
#     if embedding_vector is not None:
#         embedding_matrix[i] = embedding_vector

"""### 3.3.3 Paragram"""

from tqdm import tqdm
import numpy as np
P_embeddings_index = {}
f = open('/content/drive/MyDrive/L645/data/paragram_300_sl999.txt', encoding='utf8')  #300d

for line in tqdm(f):
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:])
    P_embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(P_embeddings_index))

embedding_matrix = np.zeros((len(word_index)+ 1, 300))
for word, i in tqdm(word_index.items()):
    embedding_vector = P_embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

"""# **4. Building models**"""

# binarize the labels for the neural network
from keras.utils import np_utils
from keras.models import Sequential
from tensorflow.keras.layers import Embedding
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from keras import layers
from keras.layers import LSTM, SimpleRNN, GRU
from keras.layers import SpatialDropout1D, Bidirectional
from keras.layers.core import Dense, Activation, Dropout

y_train_encode = np_utils.to_categorical(y_train)
y_test_encode = np_utils.to_categorical(y_test)

"""### 4.1 Simple RNN"""

# Build the RNN model
def build_RNN_model():
    model = Sequential()
    model.add(Embedding(len(word_index) + 1, 300, weights = [embedding_matrix], input_length = MAX_LENGTH, trainable = False))
    model.add(SpatialDropout1D(0.2))
    model.add(Bidirectional(SimpleRNN(300, dropout = 0.2, recurrent_dropout=0.2)))
    model.add(Dense(300, activation = 'relu'))
    model.add(Dropout(0.2))
    model.add(Dense(3))
    model.add(Activation('softmax'))
    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    return model

# Train the model
model_rnn = build_RNN_model()
history_rnn = model_rnn.fit(X_train_padding, y=y_train_encode, batch_size=512, epochs=15, verbose=1, validation_data=(X_test_padding, y_test_encode))

"""### 4.2 LSTM"""

# Build LSTM model

def build_LSTM_model():
    model = Sequential()
    # model.add(Embedding(len(word_index) + 1, 100, weights = [embedding_matrix], input_length = MAX_LENGTH, trainable = False))
    model.add(Embedding(len(word_index) + 1, 300, weights = [embedding_matrix], input_length = MAX_LENGTH, trainable = False))
    model.add(SpatialDropout1D(0.2))
    model.add(Bidirectional(LSTM(300, dropout = 0.2, recurrent_dropout=0.2)))
    model.add(Dense(300, activation = 'relu'))
    model.add(Dropout(0.2))
    model.add(Dense(3))
    model.add(Activation('softmax'))
    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    return model

# Train the model
model_lstm = build_LSTM_model()
history_lstm = model_lstm.fit(X_train_padding, y=y_train_encode, batch_size=512, epochs=15, verbose=1, validation_data=(X_test_padding, y_test_encode))

"""### 4.3 GRU"""

# Build GRU model
def build_GRU_model():
    model = Sequential()
    model.add(Embedding(len(word_index) + 1, 300, weights = [embedding_matrix], input_length = MAX_LENGTH, trainable = False))
    model.add(SpatialDropout1D(0.2))
    model.add(Bidirectional(GRU(300, dropout = 0.2, recurrent_dropout=0.2)))
    model.add(Dense(300, activation = 'relu'))
    model.add(Dropout(0.2))
    model.add(Dense(3))
    model.add(Activation('softmax'))
    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    return model

# Train the model
model_gru = build_GRU_model()
history_gru = model_gru.fit(X_train_padding, y=y_train_encode, batch_size=512, epochs=15, verbose=1, validation_data=(X_test_padding, y_test_encode))

"""# **5. Module Validation and Evaluation**"""

# plot the training and validation accuracy per epoch of a model.
def performance_history(history, model_name, image_path, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_' + string])
    plt.xlabel('Epochs')
    plt.ylabel(string)
    plt.legend([string, 'val_' + string])
    plt.title(model_name + ' ' + string + ' per epoch')
    # Save the figure
    plt.savefig(image_path + model_name + ' ' + string + ' per epoch.png', dpi = 600)
    # Display the figure
    plt.show()

from sklearn import metrics
import matplotlib.pyplot as plt
# generate the performance report of a model
def performance_report(model, model_name, image_path, testX, testy):

    time = date.today()
    
    yhat_probs=model.predict(testX) 
    yhat_classes = np.argmax(yhat_probs, axis = 1)
    ytest = np.argmax(testy, axis = 1)
    
    accuracy = accuracy_score(ytest, yhat_classes)  # (tp + tn) / (p + n)
    precision = precision_score(ytest, yhat_classes, average = 'macro') # tp / (tp + fp)
    recall = recall_score(ytest, yhat_classes, average = 'macro') # tp / (tp + fn)
    f1 = f1_score(ytest, yhat_classes, average = 'macro') # 2 tp / (2 tp + fp + fn)
    
    # confusion matrix
    cf_matrix = confusion_matrix(ytest, yhat_classes)
    cf_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cf_matrix, display_labels = ['Hate', 'Offensive', 'Neither'])
    # plot the confusion matrix
    cf_display.plot()
    # plt.show()
    # save the confusion matrix figure
    plt.savefig(image_path + model_name + ' performance_report.png', dpi=600)
    # figure.savefig(image_path + model_name + ' performance_report.png', dpi=600, transparent=True, bbox_inches='tight')

    total_cost_df = pd.DataFrame(
            columns=['Time', 'Model Name', 'Precision', 'Recall', 'F1 Score', 'Accuracy'])

    total_cost_df = total_cost_df.append(
            {'Time': time, 
             'Precision': precision,
             'Model Name': model_name,
             'Recall': recall, 
             'F1 Score': f1,
             'Accuracy':accuracy},
            ignore_index=True)
    return total_cost_df

## Create folders to save the models and plot
import os 

image_path = '/content/drive/MyDrive/L645/project/paragram/images/'
model_path = '/content/drive/MyDrive/L645/project/paragram/'

# Create image and model folder if they don't exist
def create_directry(path):
    # Check whether the specified path exists or not
    isExist = os.path.exists(path)
    if not isExist:
      # Create a new directory because it does not exist 
      os.makedirs(path)

# Create image path
create_directry(image_path)

# Create model path
create_directry(model_path)

# Compare models
from datetime import date
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
rnn_report = performance_report(model_rnn, "Simple RNN", image_path, X_test_padding, y_test_encode)
rnn_report

lstm_report = performance_report(model_lstm, "LSTM", image_path, X_test_padding, y_test_encode)
lstm_report

GRU_report = performance_report(model_gru, "GRU", image_path, X_test_padding, y_test_encode)
GRU_report

performance_history(history_lstm, "Simple RNN", image_path, 'accuracy')
performance_history(history_lstm, "Simple RNN", image_path, 'loss')

performance_history(history_lstm, "LSTM", image_path, 'accuracy')
performance_history(history_lstm, "LSTM", image_path, 'loss')

performance_history(history_lstm, "GRU", image_path, 'accuracy')
performance_history(history_lstm, "GRU", image_path, 'loss')

"""# **6. Add class weight to handel unbalanced dataset**

### 6.1 Calculate class weight using fomula: w_j = n_samples / (n_classes * n_samples_j)
"""

# Total number of samples
n_samples = len(dataset)

# Total number of classes
n_classes = len(pd.unique(dataset['class']))

# Total number of samples in each class
n_sample_0 = sum(dataset['class'] == 0)
n_sample_1 = sum(dataset['class'] == 1)
n_sample_2 = sum(dataset['class'] == 2)

# The weights of each calss
w_0 = n_samples / (n_classes * n_sample_0)
w_1 = n_samples / (n_classes * n_sample_1)
w_2 = n_samples / (n_classes * n_sample_2)

# Class weights
class_weights = {
    0: w_0,
    1: w_1,
    2: w_2
}

class_weights

"""6.2 Train GRU model with class weight """

# #Train the model
# model_GRU_weighted = build_GRU_model()
# history_GRU_weighted = model_GRU_weighted.fit(X_train_padding, y=y_train_encode, batch_size=512, epochs=15, verbose=1, validation_data=(X_test_padding, y_test_encode), class_weight = class_weights)

# model_GRU_weighted.summary()
# performance_report(model_GRU_weighted, "Weighted GRU", image_path, X_test_padding, y_test_encode)

# # Accuracy and loss of weighted GRU model
# performance_history(history_GRU_weighted, "Weighted GRU", image_path, 'accuracy')
# performance_history(history_GRU_weighted, "Weighted GRU", image_path, 'loss')

# Train the model
model_lstm_weighted = build_LSTM_model()
history_lstm_weighted = model_lstm_weighted.fit(X_train_padding, y=y_train_encode, batch_size=512, epochs=15, verbose=1, validation_data=(X_test_padding, y_test_encode), class_weight = class_weights)

model_lstm_weighted.summary()
performance_report(model_lstm_weighted, "Weighted LSTM", image_path, X_test_padding, y_test_encode)

# Accuracy and loss of weighted LSTM model
performance_history(history_lstm_weighted, "Weighted LSTM", image_path, 'accuracy')
performance_history(history_lstm_weighted, "Weighted LSTM", image_path, 'loss')

"""### 6.3 Save the models"""

def save_model(model, model_name, model_path):
    model.save(model_path + model_name)

save_model(model_rnn, "Simple_RNN_Model", model_path)
save_model(model_lstm, "LSTM_Model", model_path)
save_model(model_gru, "GRU_Model", model_path)
save_model(model_lstm_weighted, "Weighted_LSTM_Model", model_path)

"""# **7. Test the model on new data**

### 7.1 Load the models
"""

from tensorflow.keras.models import load_model
weighted_LSTM_model = load_model('/content/drive/MyDrive/L645/project/paragram/Weighted_LSTM_Model')
lstm_model = load_model('/content/drive/MyDrive/L645/project/paragram/LSTM_Model')

"""### 7.2 Prepare the data"""

# Tweet to predict
tweet = input()

# Convert into dataframe
test_data = pd.DataFrame(columns=['tweet'])
test_data.loc[0, 'tweet'] = tweet

# Replace contractions from tweets
test_data["tweet"] = test_data.tweet.apply(replace_contraction)

# Clean the tweets
test_data["tweet"] = test_data.tweet.apply(clean_tweets)

# Remove stop words
test_data['tweet'] = test_data.tweet.apply(remove_stop_words)

# Lemmatize
test_data['tweet'] = list(map(lemmatize_tweets, test_data['tweet']))

# Text to sequences
test_data_sequence = tokenizer.texts_to_sequences(test_data['tweet'])

# Padding
test_data_padding = pad_sequences(test_data_sequence, maxlen = MAX_LENGTH, padding = 'post', truncating = 'post')

"""### 7.3 Predict"""

# This function predicts the class of a provided tweet
def predict_tweet(model, data):
    predictions = model.predict(data)
    prediction = np.argmax(predictions, axis = 1)
    if prediction == 0:
        print("Hate Speech")
    elif prediction == 1:
        print("Offensive Language")
    else:
        print("Neither")
        
# Predict class of a tweet

# Prediction using unweighted lstm model
predict_tweet(lstm_model, test_data_padding)

# Prediction using weighted lstm model
predict_tweet(weighted_LSTM_model, test_data_padding)